{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sigmoid\n",
    "def sigmoid(Z):\n",
    "    \n",
    "    Z[Z > 709] =  709 #prevent np.exp overflow\n",
    "    Z[Z <-709] =  0   #prevent np.exp overflow\n",
    "    \n",
    "    A = np.where(Z >= 0, \n",
    "                    1. / (1. + np.exp(-Z)), \n",
    "                    np.exp(Z) / (1. + np.exp(Z))) # Activation\n",
    "    C = Z                       # Cache\n",
    "    \n",
    "    return A , C \n",
    "\n",
    "\n",
    "def sigmoid_prime(dA, C): ## Derivative and Cache\n",
    "    \n",
    "    Z = C                 ## Cache\n",
    "    \n",
    "    S = 1. / (1. +np.exp(-Z))\n",
    "    ds = S * (1-S)\n",
    "    \n",
    "    dZ = dA * ds\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### RELU\n",
    "def relu(Z, alpha=0.0000000001):\n",
    "    C = Z                       # Cache\n",
    "    A = np.maximum(alpha*Z,Z)     # Activation\n",
    "    \n",
    "    return A , C \n",
    "\n",
    "\n",
    "def relu_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    #dZ = dZ * 2\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### tanH\n",
    "def tanh(Z):\n",
    "    \n",
    "    C = Z                       # Cache\n",
    "    A = np.tanh(Z)              # Activation\n",
    "    \n",
    "    return A , C\n",
    "\n",
    "\n",
    "def tanh_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    dZ = 1-np.tanh(dA)**2\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "### Identity\n",
    "def identity(Z):\n",
    "    \n",
    "    C = Z                       # Cache\n",
    "    A = Z                       # Activation\n",
    "    \n",
    "    return A , C\n",
    "\n",
    "\n",
    "def identity_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    dZ = dA\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### SoftMax\n",
    "def softmax(Z, axis=1):\n",
    "    \n",
    "    C = Z\n",
    "    A = np.exp(x) / np.sum(np.exp(x), axis = axis, keepdims = True)\n",
    "    \n",
    "    return A , C\n",
    "\n",
    "def softmax_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    s = dA.reshape(-1,1)\n",
    "    dZ = p.diagflat(s) - np.dot(s, s.T)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss L1\n",
    "def L1(yhat, y):\n",
    "    \n",
    "    loss = np.sum(abs(y-yhat))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "### Loss L2\n",
    "def L2(yhat, y):\n",
    "    \n",
    "    loss = np.dot( (y-yhat),(y-yhat) )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "### Loss Mean Square Error\n",
    "def mse(yhat, y):\n",
    "    \n",
    "    loss = np.mean(np.power(y-yhat, 2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def mse_prime(yhat, y):\n",
    "    \n",
    "    loss_prime = (2 * (y-yhat)) /y.size\n",
    "    \n",
    "    return loss_prime\n",
    "\n",
    "\n",
    "### Loss cross entopy cost\n",
    "def cross_entropy_cost(yhat, y):\n",
    "    \n",
    "    m = y.shape[1]\n",
    "    \n",
    "    cost = np.squeeze( (1./m) * (-np.dot(y,np.log(yhat).T) - np.dot(1-y, np.log(1-yhat).T)) )\n",
    "    \n",
    "    return cost\n",
    "\n",
    "### Loss cross entopy cost\n",
    "def cross_entropy_cost_prime(yhat, y):\n",
    "    \n",
    "    dA = - (np.divide(y, yhat) - np.divide(1 - y, 1 - yhat))\n",
    "    \n",
    "    return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "        'Identity':(identity,identity_prime),\n",
    "        'Sigmoid' :(sigmoid ,sigmoid_prime ),\n",
    "        'Relu'    :(relu    ,relu_prime    ),\n",
    "        'Tanh'    :(tanh    ,tanh_prime    ),\n",
    "        'Softmax' :(softmax ,softmax_prime )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Unit:\n",
    "    def __init__(self, activation_fn='Identity'):\n",
    "        \n",
    "        self.n_X = None  # Input_size\n",
    "        self.n_A = None  # Output_size\n",
    "        \n",
    "        self.activation_f = activation_map[activation_fn][0]\n",
    "        self.activation_b = activation_map[activation_fn][1]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, dA, cache):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nn_unit import NN_Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedUnit(NN_Unit):\n",
    "    \n",
    "    def __init__(self, n_X, n_A, activation_fn): # num Input , num_output\n",
    "        super(FullyConnectedUnit, self).__init__(activation_fn)\n",
    "        \n",
    "        self.n_X = n_X\n",
    "        self.n_A = n_A\n",
    "        \n",
    "        self.W = np.random.randn(n_A, n_X) * 0.01\n",
    "        self.B = np.zeros((n_A, 1))        \n",
    "                \n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        self.Z = self.W.dot(self.X) + self.B\n",
    "        \n",
    "        Z_cache = (self.X, self.W, self.B)\n",
    "        \n",
    "        self.A, A_cache = self.activation_f(self.Z)\n",
    "        return self.A, (Z_cache, A_cache)\n",
    "\n",
    "    def backward(self, dA, cache):\n",
    "        \n",
    "        Z_cache, A_cache = cache\n",
    "        \n",
    "        dZ = self.activation_b(dA, A_cache)\n",
    "        \n",
    "        A , W , B = Z_cache        \n",
    "        \n",
    "        m = A.shape[1]\n",
    "        \n",
    "        dW = 1./m * np.dot(dZ,A.T)\n",
    "        dB = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(W.T,dZ)\n",
    "        \n",
    "        return dA, dW, dB\n",
    "    \n",
    "    def init(self):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        self.W = np.random.randn(self.n_A, self.n_X) / np.sqrt(self.n_X)\n",
    "        self.B = np.zeros((self.n_A, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2dUnit(NN_Unit):\n",
    "    \n",
    "    # n_A is here pool_filter_size\n",
    "    def __init__(self, n_X, n_Y, n_A, stride=2, activation_fn='Identity'): # num Input , num_output\n",
    "        super(FullyConnectedUnit, self).__init__(activation_fn)\n",
    "        \n",
    "        self.n_X = n_X\n",
    "        self.n_X = n_Y\n",
    "        self.n_A = n_A\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def backward(self, dA, cache):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def init(self):\n",
    "        \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Layer:\n",
    "    def __init__(self):\n",
    "        self.nn_unit = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, dA, cache):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, grads, lr=0.001):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from layer import NN_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(NN_Layer):\n",
    "    \n",
    "    def __init__(self, num_in, num_out, activation_fn): # num Input , num_output\n",
    "        \n",
    "        self.ins  = num_in\n",
    "        self.outs = num_out\n",
    "        self.nn_unit = FullyConnectedUnit(num_in,num_out,activation_fn)\n",
    "                \n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.A, (Z_cache, A_cache) = self.nn_unit.forward(X)\n",
    "        \n",
    "        return self.A, (Z_cache, A_cache)\n",
    "\n",
    "    def backward(self, dA, cache):\n",
    "        \n",
    "        dA, dW, dB = self.nn_unit.backward(dA, cache)\n",
    "        \n",
    "        return dA, dW, dB\n",
    "    \n",
    "    def update(self, dW, dB, lr=0.01):\n",
    "        self.nn_unit.W = self.nn_unit.W - (lr * dW)\n",
    "        self.nn_unit.B = self.nn_unit.B - (lr * dB)\n",
    "        \n",
    "    def init(self):\n",
    "        self.nn_unit.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "import h5py\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Network:\n",
    "\n",
    "    def __init__(self, l_rate=0.01):\n",
    "        \n",
    "        self.layers = []\n",
    "        self.l_rate = l_rate\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        num_out, num_in = layer.outs, layer.ins\n",
    "        self.layers.append(layer)\n",
    "        layer.init()\n",
    "                \n",
    "    def train(self, X, Y, epochs= 100):\n",
    "        for e in range(0,epochs):\n",
    "            A = X\n",
    "            caches = []\n",
    "            for idx, l in enumerate(self.layers):\n",
    "                A, cache = l.forward(A)\n",
    "                caches.append(cache)\n",
    "            \n",
    "            cost = cross_entropy_cost(A, Y)\n",
    "            \n",
    "            dA = cross_entropy_cost_prime(A, Y)\n",
    "                \n",
    "            cnt = len(self.layers)\n",
    "            Y = Y.reshape(A.shape)\n",
    "            \n",
    "            for i in reversed(range(0, cnt)):\n",
    "                dA, dW, dB = self.layers[i].backward(dA, caches[i])\n",
    "                self.layers[i].update(dW, dB)\n",
    "            \n",
    "\n",
    "            if e%100 == 0 and e != 0:\n",
    "                print(f\"Cost at epoch {e} is {cost}\")\n",
    "            \n",
    "    def predict(self, X):\n",
    "        A = X\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            A, cache = l.forward(A)\n",
    "        \n",
    "        A[A>=0.5]= 1\n",
    "        A[A< 0.5]= 0\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        Yhat = self.predict(X).astype(int)\n",
    "        print(\"Accuracy: \"  + str(np.sum((Yhat.astype(int) == Y)/Y.shape[1])))\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc0 = FullyConnectedLayer(12288,512,'Relu')\n",
    "fc1 = FullyConnectedLayer(512,16,'Relu')\n",
    "fcf = FullyConnectedLayer(16,1,'Sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_layer(fc0)\n",
    "model.add_layer(fc1)\n",
    "model.add_layer(fcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 100 is 0.49238334060411304\n",
      "Cost at epoch 200 is 0.39120726086224006\n",
      "Cost at epoch 300 is 0.3016951246630109\n",
      "Cost at epoch 400 is 0.20489188247015286\n",
      "Cost at epoch 500 is 0.12428382078720963\n",
      "Cost at epoch 600 is 0.07062776707009956\n",
      "Cost at epoch 700 is 0.04327363162376949\n",
      "Cost at epoch 800 is 0.030559793125277006\n",
      "Cost at epoch 900 is 0.022929424092737596\n",
      "Cost at epoch 1000 is 0.018032471207089333\n",
      "Cost at epoch 1100 is 0.014628058818493076\n",
      "Cost at epoch 1200 is 0.012147554720710663\n",
      "Cost at epoch 1300 is 0.01027773085121551\n",
      "Cost at epoch 1400 is 0.008837663966558466\n",
      "Cost at epoch 1500 is 0.0077122095565339225\n",
      "Cost at epoch 1600 is 0.006803003702345492\n",
      "Cost at epoch 1700 is 0.0060584105745395325\n",
      "Cost at epoch 1800 is 0.005441119540565534\n",
      "Cost at epoch 1900 is 0.0049267002194969284\n",
      "Cost at epoch 2000 is 0.004486548090775295\n",
      "Cost at epoch 2100 is 0.004110198636251039\n",
      "Cost at epoch 2200 is 0.0037838031502715735\n",
      "Cost at epoch 2300 is 0.0035006255539303736\n",
      "Cost at epoch 2400 is 0.0032521531255693585\n"
     ]
    }
   ],
   "source": [
    "model.train(train_x, train_y, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.evaluate(test_x, test_y)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
