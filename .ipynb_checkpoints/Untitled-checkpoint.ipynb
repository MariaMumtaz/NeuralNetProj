{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sigmoid\n",
    "def sigmoid(Z):\n",
    "    \n",
    "    Z[Z > 709] =  709 #prevent np.exp overflow\n",
    "    Z[Z <-709] =  0   #prevent np.exp overflow\n",
    "    \n",
    "    A = np.where(Z >= 0, \n",
    "                    1. / (1. + np.exp(-Z)), \n",
    "                    np.exp(Z) / (1. + np.exp(Z))) # Activation\n",
    "    C = Z                       # Cache\n",
    "    \n",
    "    return A , C \n",
    "\n",
    "\n",
    "def sigmoid_prime(dA, C): ## Derivative and Cache\n",
    "    \n",
    "    Z = C                 ## Cache\n",
    "    \n",
    "    S = 1./(1.+np.exp(-Z))\n",
    "    ds = S * (1-S)\n",
    "    \n",
    "    dZ = dA * S\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### RELU\n",
    "def relu(Z, alpha=0.00001):\n",
    "    C = Z                       # Cache\n",
    "    A = np.maximum(alpha*Z,Z)     # Activation\n",
    "    \n",
    "    return A , C \n",
    "\n",
    "\n",
    "def relu_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    dZ[Z >  0] = 1\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### tanH\n",
    "def tanh(Z):\n",
    "    \n",
    "    C = Z                       # Cache\n",
    "    A = np.tanh(Z)              # Activation\n",
    "    \n",
    "    return A , C\n",
    "\n",
    "\n",
    "def tanh_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    dZ = 1-np.tanh(dA)**2\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "### Identity\n",
    "def identity(Z):\n",
    "    \n",
    "    C = Z                       # Cache\n",
    "    A = Z                       # Activation\n",
    "    \n",
    "    return A , C\n",
    "\n",
    "\n",
    "def identity_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    dZ = dA\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "### SoftMax\n",
    "def softmax(Z, axis=1):\n",
    "    \n",
    "    C = Z\n",
    "    A = np.exp(x) / np.sum(np.exp(x), axis = axis, keepdims = True)\n",
    "    \n",
    "    return A , C\n",
    "\n",
    "def softmax_prime(dA, C):\n",
    "    \n",
    "    Z = C                      ## Cache\n",
    "    \n",
    "    s = dA.reshape(-1,1)\n",
    "    dZ = p.diagflat(s) - np.dot(s, s.T)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss L1\n",
    "def L1(yhat, y):\n",
    "    \n",
    "    loss = np.sum(abs(y-yhat))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "### Loss L2\n",
    "def L2(yhat, y):\n",
    "    \n",
    "    loss = np.dot( (y-yhat),(y-yhat) )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "### Loss Mean Square Error\n",
    "def mse(yhat, y):\n",
    "    \n",
    "    loss = np.mean(np.power(y-yhat, 2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def mse_prime(yhat, y):\n",
    "    \n",
    "    loss_prime = (2 * (y-yhat)) /y.size\n",
    "    \n",
    "    return loss_prime\n",
    "\n",
    "\n",
    "### Loss cross entopy cost\n",
    "def cross_entropy_cost(yhat, y):\n",
    "    \n",
    "    m = y.shape[1]\n",
    "    \n",
    "    cost = np.squeeze( (1./m) * (-np.dot(y,np.log(yhat).T) - np.dot(1-y, np.log(1-yhat).T)) )\n",
    "    \n",
    "    return cost\n",
    "\n",
    "### Loss cross entopy cost\n",
    "def cross_entropy_cost_prime(yhat, y):\n",
    "    \n",
    "    dA = - (np.divide(y, yhat) - np.divide(1 - y, 1 - yhat))\n",
    "    \n",
    "    return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "        'Identity':(identity,identity_prime),\n",
    "        'Sigmoid' :(sigmoid ,sigmoid_prime ),\n",
    "        'Relu'    :(relu    ,relu_prime    ),\n",
    "        'Tanh'    :(tanh    ,tanh_prime    ),\n",
    "        'Softmax' :(softmax ,softmax_prime )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Unit:\n",
    "    def __init__(self, activation_fn='Identity'):\n",
    "        \n",
    "        self.n_X = None  # Input_size\n",
    "        \n",
    "        self.activation_f = activation_map[activation_fn][0]\n",
    "        self.activation_b = activation_map[activation_fn][1]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, dA, cache):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init(self, n_X, n_A, n_L):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nn_unit import NN_Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedUnit(NN_Unit):\n",
    "    \n",
    "    def __init__(self, n_X, n_A, activation_fn): # num Input , num_output\n",
    "        super(FullyConnectedUnit, self).__init__(activation_fn)\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        self.n_X = n_X\n",
    "        \n",
    "        self.W = np.random.randn(n_A, n_X) * 0.01\n",
    "        self.B = np.zeros((n_A, 1))\n",
    "        \n",
    "        print(f\"n_A {n_A}\")\n",
    "        print(f\"n_X {n_X}\")\n",
    "        print(self.W)\n",
    "        print(self.B)\n",
    "        \n",
    "                \n",
    "    def forward(self, X):\n",
    "        \n",
    "        print(self.W)\n",
    "        print(self.B)\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        self.Z = self.W.dot(self.X) + self.B\n",
    "        \n",
    "        Z_cache = (self.X, self.W, self.B)\n",
    "        \n",
    "        self.A, A_cache = self.activation_f(self.Z)\n",
    "        \n",
    "        return self.A, (Z_cache, A_cache)\n",
    "\n",
    "    def backward(self, dA, cache):\n",
    "        \n",
    "        Z_cache, A_cache = cache\n",
    "        \n",
    "        dZ = self.activation_b(dA, A_cache)\n",
    "        \n",
    "        A , W , B = Z_cache        \n",
    "        \n",
    "        m = A.shape[1]\n",
    "        \n",
    "        dW = 1./m * np.dot(dZ,A.T)\n",
    "        dB = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = np.dot(W.T,dZ)\n",
    "        \n",
    "        return dA, dW, dB\n",
    "    \n",
    "    def init(self, n_X, n_A, n_L):\n",
    "        self.W = np.random.randn(n_A, n_X) / np.sqrt(n_L)\n",
    "        self.B = np.zeros((n_A, 1))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Layer:\n",
    "    def __init__(self):\n",
    "        self.nn_unit = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, dA, cache):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, grads, lr=0.001):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init(self, n_L):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from layer import NN_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(NN_Layer):\n",
    "    \n",
    "    def __init__(self, num_in, num_out, activation_fn): # num Input , num_output\n",
    "        \n",
    "        self.ins  = num_in\n",
    "        self.outs = num_out\n",
    "        self.nn_unit = FullyConnectedUnit(num_in,num_out,activation_fn)\n",
    "                \n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.A, (Z_cache, A_cache) = self.nn_unit.forward(X)\n",
    "        \n",
    "        return self.A, (Z_cache, A_cache)\n",
    "\n",
    "    def backward(self, dA, cache):\n",
    "        \n",
    "        dA, dW, dB = self.nn_unit.backward(dA, cache)\n",
    "        \n",
    "        return dA, dW, dB\n",
    "    \n",
    "    def update(self, dW, dB, lr=0.01):\n",
    "        self.nn_unit.W = self.nn_unit.W - (lr * dW)\n",
    "        self.nn_unit.B = self.nn_unit.B - (lr * dB)\n",
    "        \n",
    "    def init(self, n_L):\n",
    "        self.nn_unit.init(self.ins, self.outs, n_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Network:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        num_out, num_in = layer.outs, layer.ins\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def train(self, X, Y, epochs= 100):\n",
    "        for e in range(0,epochs):\n",
    "            A = X\n",
    "            caches = []\n",
    "            for idx, l in enumerate(self.layers):\n",
    "                A, cache = l.forward(A)\n",
    "                caches.append(cache)\n",
    "            \n",
    "            cost = cross_entropy_cost(A, Y)\n",
    "            dA = cross_entropy_cost_prime(A, Y)\n",
    "            \n",
    "            cnt = len(self.layers)\n",
    "            Y = Y.reshape(A.shape)\n",
    "            \n",
    "            for i in reversed(range(0, cnt)):\n",
    "                dA, dW, dB = self.layers[i].backward(dA, caches[i])\n",
    "                self.layers[i].update(dW, dB)\n",
    "\n",
    "            print(f\"-------------- {e}\")\n",
    "            print(f\"-------------- \")\n",
    "            if e%100 == 0 and e != 0:\n",
    "                print(f\"Cost at epoch {e} is {cost}\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        A = X\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            A, cache = l.forward(A)\n",
    "        print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_A 7\n",
      "n_X 12288\n",
      "[[ 0.01624345 -0.00611756 -0.00528172 ... -0.00527214 -0.0038034\n",
      "   0.00949412]\n",
      " [ 0.01009231  0.00229889 -0.00664099 ...  0.00689859 -0.00488322\n",
      "   0.0020761 ]\n",
      " [-0.0035634  -0.00195481  0.00636803 ...  0.00822751 -0.00104425\n",
      "  -0.00657957]\n",
      " ...\n",
      " [-0.00315398  0.0124543  -0.01304592 ...  0.00370126  0.01033177\n",
      "  -0.00896044]\n",
      " [ 0.00487913  0.00350058 -0.00084749 ... -0.00559281  0.00325022\n",
      "   0.00231429]\n",
      " [ 0.00534074  0.02512388 -0.01044755 ...  0.0216518  -0.00842579\n",
      "   0.00359795]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "n_A 1\n",
      "n_X 7\n",
      "[[ 0.01624345 -0.00611756 -0.00528172 -0.01072969  0.00865408 -0.02301539\n",
      "   0.01744812]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "fc0 = FullyConnectedLayer(12288,7,'Relu')\n",
    "#fc1 = FullyConnectedLayer(512,16,'Relu')\n",
    "#fc2 = FullyConnectedLayer(256,128,'Relu')\n",
    "#fc3 = FullyConnectedLayer(128,64,'Relu')\n",
    "#fc4 = FullyConnectedLayer(64,32,'Relu')\n",
    "#fc5 = FullyConnectedLayer(32,16,'Relu')\n",
    "fcf = FullyConnectedLayer(7,1,'Sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_layer(fc0)\n",
    "#model.add_layer(fc1)\n",
    "#model.add_layer(fc2)\n",
    "#model.add_layer(fc3)\n",
    "#model.add_layer(fc4)\n",
    "#model.add_layer(fc5)\n",
    "model.add_layer(fcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00379768 -0.00385948 -0.00345322 ... -0.00330145 -0.00343037\n",
      "  -0.0026822 ]\n",
      " [-0.00376859 -0.00389774 -0.00338244 ... -0.00329301 -0.00336374\n",
      "  -0.00270092]\n",
      " [-0.00374925 -0.00393072 -0.00344285 ... -0.00330735 -0.00343654\n",
      "  -0.00265322]\n",
      " ...\n",
      " [-0.00383882 -0.00386569 -0.00341439 ... -0.00327958 -0.00335463\n",
      "  -0.00272589]\n",
      " [-0.00379776 -0.00388591 -0.00335989 ... -0.00323993 -0.00334741\n",
      "  -0.00269147]\n",
      " [-0.00375046 -0.00386726 -0.00343029 ... -0.00325462 -0.00342353\n",
      "  -0.00267463]]\n",
      "[[-0.01]\n",
      " [-0.01]\n",
      " [-0.01]\n",
      " [-0.01]\n",
      " [-0.01]\n",
      " [-0.01]\n",
      " [-0.01]]\n",
      "[[-0.00068194 -0.00065841 -0.00072186 -0.00069462 -0.00070966 -0.00071642\n",
      "  -0.00069991]]\n",
      "[[-0.00311035]]\n",
      "-------------- 0\n",
      "-------------- \n"
     ]
    }
   ],
   "source": [
    "model.train(train_x, train_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc0.nn_unit.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=X\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = FullyConnectedUnit(2,2,'Sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(1, 30)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=(X>0).astype(int)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.zeros((4,1))\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = np.array([[ 0]])\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]])\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]])\n",
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit2 = FullyConnectedUnit(2,1,'Sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, cache1 = unit.forward(X)\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A2, cache2 = unit2.forward(A1)\n",
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[0] ## X,W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[1] ## Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([[0],[1]])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Z = cache[0][1].dot(cache[0][0])\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x(np.random.rand(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
