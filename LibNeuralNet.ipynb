{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural network to recognize hand written Images\n",
    "\n",
    "This is a simple neural network constructed in pytorch to train on MINST Hand written digits data set to recognize any image with a hand written number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary utilities\n",
    "\n",
    "The project is done in pytorch using Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "\n",
    "Lets set some global variables such as seed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "valid_size = 0.1\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data can be downloaded from here:\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Four files are available on this site:\n",
    "\n",
    "- train-images-idx3-ubyte.gz:  training set images (9912422 bytes)\n",
    "- train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)\n",
    "- t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)\n",
    "- t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n",
    "\n",
    "\n",
    "the data is downloaded in data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the MINST dataset. The following code takes care of downloading, wrapping around the DataLoader and normalizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization for MNIST dataset.\n",
    "dataset_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('data/', train=True,  download=True, transform=dataset_transform)\n",
    "valid_dataset = datasets.MNIST('data/', train=True,  download=True, transform=dataset_transform)\n",
    "test_dataset  = datasets.MNIST('data/', train=False, download=True, transform=dataset_transform)\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, sampler=train_sampler, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, sampler=valid_sampler, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine a sample image using matplotlib ploting library\n",
    "\n",
    "we would get one batch from the loader of 32 images and plot in 8 x 4 grid\n",
    "\n",
    "each image is 28 x 28 x 1 meaning single channel square image with length and higet = 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "train_iterator = iter(train_loader)\n",
    "images, labels = train_iterator.next() # Batch of 64\n",
    "\n",
    "num_row = 4\n",
    "num_col = 8\n",
    "\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(train_batch_size):\n",
    "    ax = axes[i//num_col, i%num_col]\n",
    "    ax.imshow(images[i][0], cmap='gray')\n",
    "    ax.set_title('Label: {}'.format(labels[i]))\n",
    "    #save_image(images[i], \"img\"+str(i)+\".png\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "We define our neural network with with 2 sequence of 2d convolutional layers followed by RELU activationa and max pooling. At the end will ba a fully connected layer followed by softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/network.jpg\" style=\"width:800px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the dimensions of our neurons, we would use following util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_volume(W, F, S, P):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the input volume size $W$, the kernel/filter size $F$, \n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border, \n",
    "    calculate the output volume size.\n",
    "    \"\"\"\n",
    "    return int((W - F + 2*P) / S) + 1\n",
    "\n",
    "def maxpool_output_volume(W, F, S):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the input volume size $W$, the kernel/filter size $F$, \n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border, \n",
    "    calculate the output volume size.\n",
    "    \"\"\"\n",
    "    return int(np.ceil((W - F + 1) / S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv 1 output\n",
    "\n",
    "input_size   = 28\n",
    "filter_size  = 5\n",
    "stride_size  = 1\n",
    "padding_size = 0\n",
    "\n",
    "conv_out_1 = conv_output_volume(input_size,filter_size,stride_size,padding_size)\n",
    "conv_out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool 1 putput\n",
    "\n",
    "input_size   = conv_out_1\n",
    "filter_size  = 2\n",
    "stride_size  = 2\n",
    "padding_size = 0\n",
    "\n",
    "pool_out_1 = maxpool_output_volume(input_size,filter_size,stride_size)\n",
    "pool_out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv 1 output\n",
    "\n",
    "input_size   = pool_out_1\n",
    "filter_size  = 3\n",
    "stride_size  = 1\n",
    "padding_size = 0\n",
    "\n",
    "conv_out_2 = conv_output_volume(input_size,filter_size,stride_size,padding_size)\n",
    "conv_out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool 1 putput\n",
    "\n",
    "input_size   = conv_out_2\n",
    "filter_size  = 2\n",
    "stride_size  = 2\n",
    "padding_size = 0\n",
    "\n",
    "pool_out_2 = maxpool_output_volume(input_size,filter_size,stride_size)\n",
    "pool_out_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define are NN Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #input channel 1, output channel 10\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1)\n",
    "        \n",
    "        #input channel 10, output channel 20\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear(5*5*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cpu\"\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "model = CNN(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.09534903462431796, Validation Loss: 0.130242720246315, Accuracy: 96.86666666666666 \n",
      "Epoch 2, Training Loss: 0.09952499597518098, Validation Loss: 0.13484908640384674, Accuracy: 96.95 \n",
      "Epoch 3, Training Loss: 0.10101241217653814, Validation Loss: 0.0918988287448883, Accuracy: 97.66666666666667 \n",
      "Epoch 4, Training Loss: 0.0926692469791659, Validation Loss: 0.10469111055135727, Accuracy: 97.28333333333333 \n",
      "Epoch 5, Training Loss: 0.09506685922471372, Validation Loss: 0.12856677174568176, Accuracy: 96.43333333333334 \n",
      "Epoch 6, Training Loss: 0.09547453763801779, Validation Loss: 0.1312548667192459, Accuracy: 96.9 \n",
      "Epoch 7, Training Loss: 0.09482863805263654, Validation Loss: 0.118851438164711, Accuracy: 96.58333333333333 \n",
      "Epoch 8, Training Loss: 0.09471318548281635, Validation Loss: 0.10629993677139282, Accuracy: 97.38333333333334 \n",
      "Epoch 9, Training Loss: 0.08944650738073605, Validation Loss: 0.12677665054798126, Accuracy: 97.16666666666667 \n",
      "Epoch 10, Training Loss: 0.09460979041961755, Validation Loss: 0.12737025320529938, Accuracy: 96.76666666666667 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Training loss in this epoc\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Testing loss in this epoc\n",
    "    valid_loss = 0\n",
    "    \n",
    "    # accuracy this epoc\n",
    "    accuracy = 0\n",
    "    \n",
    "    # Set in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Train all images in batches\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      \n",
    "        # Set model in evaluation mode\n",
    "        model.eval()\n",
    "      \n",
    "        # Validation pass\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            outputs = model(images)\n",
    "        \n",
    "            valid_loss += criterion(outputs, labels)\n",
    "        \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy += (100 * correct / total)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    valid_losses.append(valid_loss/len(valid_loader))\n",
    "    \n",
    "    print (f'Epoch {epoch+1}, Training Loss: {train_loss/len(train_loader)}, Validation Loss: {valid_loss/len(valid_loader)}, Accuracy: {accuracy} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model: 97.07\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the model: {(100 * correct / total)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-15.3358, -15.7937, -10.3105,   0.2804, -19.3264,  -5.1699,  -7.1395,\n",
      "         -22.5846,   4.7189, -22.8633]], grad_fn=<AddmmBackward>)\n",
      "torch.return_types.max(\n",
      "values=tensor([4.7189]),\n",
      "indices=tensor([8]))\n",
      "8\n",
      "Prediction  tensor(8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input an image and test with it\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "                        transforms.Resize(28),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "    \n",
    "image_name = 'test1.jpg'\n",
    "\n",
    "image = images[0]\n",
    "image = Image.open('images/' + image_name)\n",
    "\n",
    "# Grascale\n",
    "image = image.convert('L')\n",
    "\n",
    "image = image_transform(image)\n",
    "\n",
    "\n",
    "prediction = model(image.unsqueeze(0))\n",
    "print(prediction)\n",
    "print(torch.max(prediction.data, 1))\n",
    "_, prediction = torch.max(prediction.data, 1)\n",
    "\n",
    "print(int(prediction[0]))\n",
    "print(\"Prediction \",prediction[0])\n",
    "\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
